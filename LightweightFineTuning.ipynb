{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA\n",
    "* Model: distilbert-base-uncased\n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: ealvaradob/phishing-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed6c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn -q\n",
    "!pip install peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0074b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, \\\n",
    "    DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8f4937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n"
     ]
    }
   ],
   "source": [
    "# verify the compute resource\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required function for \n",
    "def get_train_test_sets(num_examples, seed=42, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Loads phishing-dataset from Huggingface and splits into train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        seed: (int) ensures reproducable random sample\n",
    "        num_examples: (int) number of samples to split into train and test\n",
    "        test_size: (float) fraction of dataset to hold back for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        tuple of train and test datasets as Dataset objects\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = load_dataset(\"ealvaradob/phishing-dataset\", \"combined_reduced\", trust_remote_code=True).shuffle(seed=seed)\n",
    "    display(dataset)\n",
    "    \n",
    "    # turn Dataset object into a pandas df in order to get a random sample and use sklearn train_test_split\n",
    "    df = dataset['train'].to_pandas().sample(n=num_examples, random_state=seed)\n",
    "\n",
    "    # Delete the original dataset to free up memory\n",
    "    del dataset\n",
    "\n",
    "    # Run the garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # preview df\n",
    "    display(df.head())\n",
    "    print(f\"num samples: {len(df)}\")\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train, test = train_test_split(df, test_size=test_size, shuffle=True, random_state=seed)\n",
    "\n",
    "    # convert back into Dataset objects\n",
    "    train, test = Dataset.from_pandas(train, preserve_index=False), Dataset.from_pandas(test, preserve_index=False)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def tokenize_dataset(train, test, model_name=\"distilbert-base-uncased\"):\n",
    "    \"\"\"return tokenized examples from the dataset\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        \"\"\"function to map over dataset to tokenize examples\"\"\"\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    train = train.map(preprocess_function, batched=True)\n",
    "    test = test.map(preprocess_function, batched=True)\n",
    "    \n",
    "    return train, test, tokenizer\n",
    "\n",
    "\n",
    "def build_model(model_name=\"distilbert-base-uncased\", requires_grad=False, is_lora=False):\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                 num_labels=2,\n",
    "                                                 id2label={0: \"benign\", 1: \"phishing\"},\n",
    "                                                 label2id={\"benign\": 0, \"phishing\": 1},\n",
    "                                                 )\n",
    "        \n",
    "    # Ensure LoRA parameters are trainable\n",
    "    if is_lora:\n",
    "        # Freeze all the parameters of the base model\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "            \n",
    "        for name, param in model.named_parameters():\n",
    "            if \"lora\" in name.lower():\n",
    "                param.requires_grad = True\n",
    "    else:\n",
    "        # Freeze all the parameters of the base model\n",
    "        for param in model.base_model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "            \n",
    "        model.classifier\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "def build_trainer(model, train, test, tokenizer, dir_name, lr, batch_size, epochs):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./data/\" + dir_name,\n",
    "            # Set the learning rate\n",
    "            learning_rate = lr,\n",
    "            # Set the per device train batch size and eval batch size\n",
    "            per_device_train_batch_size = batch_size,\n",
    "            per_device_eval_batch_size = batch_size,\n",
    "            # Evaluate and save the model after each epoch\n",
    "            evaluation_strategy = 'epoch',\n",
    "            save_strategy = 'epoch',\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "        ),\n",
    "        train_dataset=train,\n",
    "        eval_dataset=test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7c37bcf3224c3a98760747a7df23e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355f69466c79407faa1b452912559325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea8949f5a4945dd8d951d7085122dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/521M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a364b9139e814dfcbe00cd1cc655f796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 77677\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77156</th>\n",
       "      <td>https://warriorplus.com/support/admin/password...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28465</th>\n",
       "      <td>mlssoccer.com/videos?id=21318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>Robert Harley writes:\\n&gt; Chuck Murcko wrote:&gt; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39444</th>\n",
       "      <td>On the topic ofIt is time to refinance!Your cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html ng-app=app ng-strict-di&gt;&lt;t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "77156  https://warriorplus.com/support/admin/password...      0\n",
       "28465                      mlssoccer.com/videos?id=21318      0\n",
       "12465  Robert Harley writes:\\n> Chuck Murcko wrote:> ...      0\n",
       "39444  On the topic ofIt is time to refinance!Your cr...      1\n",
       "14496  <!doctypehtml><html ng-app=app ng-strict-di><t...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 500\n"
     ]
    }
   ],
   "source": [
    "# load dataset and get train/test splits\n",
    "train, test = get_train_test_sets(num_examples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text: saliturocarpet.com/wp-content/plugins/akismet/item/s\n",
      "length of longest text string: 86153\n"
     ]
    }
   ],
   "source": [
    "# view an example\n",
    "sample = train[2]['text']\n",
    "print(f\"sample text: {sample}\")\n",
    "\n",
    "# check length of longest text string although sequence length will be capped by model limit.\n",
    "max_length = 0\n",
    "\n",
    "for i in train:\n",
    "    if len(i['text']) > max_length:\n",
    "        max_length = len(i['text'])\n",
    "    \n",
    "print(f\"length of longest text string: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b64689",
   "metadata": {},
   "source": [
    "### Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb948cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95634c33f552407aa3342ddfecb6df2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab36f396da4106bec1608b9d3df7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1026, 999, 1011, 1011, 2303, 1063, 4281, 1011, 3746, 1024, 24471, 2140, 1006, 8299, 1024, 1013, 1013, 4871, 1012, 12625, 26745, 4168, 1012, 4012, 1013, 4871, 1013, 3277, 1013, 2327, 1011, 2157, 1012, 21025, 2546, 1007, 1025, 3609, 1024, 1001, 27533, 2546, 14142, 1065, 1037, 1024, 4957, 1063, 3609, 1024, 1001, 27533, 2546, 14142, 1025, 3793, 1011, 11446, 1024, 2104, 4179, 1025, 15489, 1011, 3635, 1024, 3671, 1065, 1037, 1024, 4716, 1063, 3609, 1024, 1001, 27533, 2546, 14142, 1025, 3793, 1011, 11446, 1024, 2104, 4179, 1025, 15489, 1011, 3635, 1024, 3671, 1065, 1037, 1024, 3161, 1063, 3609, 1024, 3897, 1025, 3793, 1011, 11446, 1024, 3904, 1065, 1037, 1024, 25215, 2099, 1063, 3609, 1024, 3897, 1025, 3793, 1011, 11446, 1024, 3904, 1065, 1052, 1012, 2516, 1063, 4281, 1024, 1001, 1039, 2575, 2683, 15878, 2546, 1025, 3675, 1011, 3953, 1024, 1001, 1018, 2546, 2475, 2546, 2549, 2050, 1018, 2361, 2595, 5024, 1025, 3675, 1011, 2187, 1024, 1001, 1041, 2581, 2094, 2629, 2063, 2549, 1018, 2361, 2595, 5024, 1025, 3675, 1011, 2157, 1024, 1001, 1018, 2546, 2475, 2546, 2549, 2050, 1018, 2361, 2595, 5024, 1025, 3675, 1011, 2327, 1024, 1001, 1041, 2581, 2094, 2629, 2063, 2549, 1018, 2361, 2595, 5024, 1025, 3609, 1024, 1001, 27533, 2546, 14142, 1025, 15489, 1011, 2155, 1024, 5021, 20344, 5796, 1010, 29461, 25987, 3388, 5796, 1010, 2002, 2140, 19510, 5555, 1010, 9342, 2140, 1025, 15489, 1011, 2946, 1024, 2403, 13876, 1025, 15489, 1011, 3635, 1024, 3671, 1065, 1052, 1012, 2217, 8237, 1063, 4281, 1024, 1001, 1041, 2581, 2094, 2629, 2063, 2549, 1025, 3675, 1011, 3953, 1024, 1001, 1039, 2575, 2683, 15878, 2546, 1017, 2361, 2595, 5024, 1025, 3675, 1011, 2187, 1024, 1001, 21461, 4246, 4246, 1017, 2361, 2595, 5024, 1025, 3675, 1011, 2157, 1024, 1001, 1039, 2575, 2683, 15878, 2546, 1017, 2361, 2595, 5024, 1025, 3675, 1011, 2327, 1024, 1001, 21461, 4246, 4246, 1017, 2361, 2595, 5024, 1025, 3609, 1024, 1001, 27533, 2546, 14142, 1025, 15489, 1011, 2155, 1024, 5021, 20344, 5796, 1010, 29461, 25987, 3388, 5796, 1010, 2002, 2140, 19510, 5555, 1010, 9342, 2140, 1025, 15489, 1011, 2946, 1024, 2184, 13876, 1025, 15489, 1011, 3635, 1024, 7782, 1025, 3793, 1011, 25705, 1024, 2415, 1065, 1012, 24471, 2140, 1063, 15489, 1011, 2946, 1024, 1022, 13876, 1025, 15489, 1011, 2155, 1024, 2310, 26992, 2050, 1010, 11937, 23393, 2050, 1010, 9342, 2140, 1065, 1011, 1011, 1028, 2526, 2692, 2581, 15136, 12625, 26745, 4168, 6627, 8325, 2050, 5718, 1012, 2324, 1012, 2526, 25781, 2890, 6442, 2562, 2115, 4773, 2609, 2551, 3435, 1998, 6047, 2007, 11087, 2094, 3619, 1012, 2062, 10539, 2836, 2965, 2062, 4026, 1010, 2062, 4341, 1010, 1998, 2062, 11372, 1012, 21100, 8162, 3401, 2115, 1040, 3619, 2968, 1998, 3828, 1012, 3937, 1040, 3619, 2005, 2069, 1002, 1015, 1012, 4002, 1037, 3204, 1012, 11562, 2182, 2000, 2424, 2041, 2062, 1012, 1045, 2245, 1045, 17776, 2026, 12418, 6113, 3351, 20652, 11020, 8241, 7483, 2043, 2009, 4188, 2000, 9573, 3294, 1012, 1045, 11780, 1037, 2951, 12436, 2278, 2013, 1037, 2767, 1998, 2787, 2000, 4550, 2035, 1997, 1996, 6497, 1998, 4937, 2606, 2013, 2026, 3001, 1010, 2059, 2044, 28667, 18256, 11873, 2673, 1010, 102]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the train and test splits\n",
    "train, test, tokenizer = tokenize_dataset(train, test)\n",
    "\n",
    "# Show first example of tokenized training set\n",
    "print(train[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fa76a",
   "metadata": {},
   "source": [
    "### Load and setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb64f0ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ab972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685513</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/phishing_or_benign/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=0.6840260314941407, metrics={'train_runtime': 10.3652, 'train_samples_per_second': 38.591, 'train_steps_per_second': 2.412, 'total_flos': 52986959462400.0, 'train_loss': 0.6840260314941407, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the huggingface trainer class to train the model\n",
    "trainer = build_trainer(model, train, test, tokenizer, dir_name='phishing_or_benign', \n",
    "                        lr=2e-5, batch_size=16, epochs=1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d115254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6855133771896362,\n",
       " 'eval_accuracy': 0.52,\n",
       " 'eval_runtime': 1.5741,\n",
       " 'eval_samples_per_second': 63.529,\n",
       " 'eval_steps_per_second': 4.447,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9ca3fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 77677\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77156</th>\n",
       "      <td>https://warriorplus.com/support/admin/password...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28465</th>\n",
       "      <td>mlssoccer.com/videos?id=21318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>Robert Harley writes:\\n&gt; Chuck Murcko wrote:&gt; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39444</th>\n",
       "      <td>On the topic ofIt is time to refinance!Your cr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>&lt;!doctypehtml&gt;&lt;html ng-app=app ng-strict-di&gt;&lt;t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "77156  https://warriorplus.com/support/admin/password...      0\n",
       "28465                      mlssoccer.com/videos?id=21318      0\n",
       "12465  Robert Harley writes:\\n> Chuck Murcko wrote:> ...      0\n",
       "39444  On the topic ofIt is time to refinance!Your cr...      1\n",
       "14496  <!doctypehtml><html ng-app=app ng-strict-di><t...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 500\n"
     ]
    }
   ],
   "source": [
    "lora_train, lora_test = get_train_test_sets(num_examples=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecbaf430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2494754da24315872cff2c0628c4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0317e6fdd1c44938480e696e33fac29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_train, lora_test, lora_tokenizer = tokenize_dataset(lora_train, lora_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "096bd84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d97a22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "lora_model = build_model(is_lora=True)\n",
    "\n",
    "\n",
    "# lora_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "#                                              num_labels=2,\n",
    "#                                              id2label={0: \"benign\", 1: \"phishing\"},\n",
    "#                                              label2id={\"benign\": 0, \"phishing\": 1},\n",
    "#                                              )\n",
    "\n",
    "# # Freeze all the parameters of the base model\n",
    "# for param in model.base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Ensure LoRA parameters are trainable\n",
    "# for name, param in lora_model.named_parameters():\n",
    "#     if \"lora\" in name.lower():\n",
    "#         param.requires_grad = True\n",
    "    \n",
    "# lora_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a PEFT config\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\", \"ffn.lin1\", \"ffn.lin2\"] #can only apply LoRA to linear layers\n",
    ")\n",
    "\n",
    "# create a PEFT model\n",
    "lora_model = get_peft_model(lora_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_trainer = build_trainer(lora_model, lora_train, lora_test, \n",
    "                             lora_tokenizer, dir_name='lora_phishing_or_benign', lr=2e-5, batch_size=16, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "653a69ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685273</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682780</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./data/lora_phishing_or_benign/checkpoint-25 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./data/lora_phishing_or_benign/checkpoint-50 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.6659337615966797, metrics={'train_runtime': 39.9672, 'train_samples_per_second': 20.016, 'train_steps_per_second': 1.251, 'total_flos': 109059883008000.0, 'train_loss': 0.6659337615966797, 'epoch': 2.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6827802062034607,\n",
       " 'eval_accuracy': 0.52,\n",
       " 'eval_runtime': 1.9532,\n",
       " 'eval_samples_per_second': 51.198,\n",
       " 'eval_steps_per_second': 3.584,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
